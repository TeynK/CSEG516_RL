# configs/ppo_config.yaml

# [기본 설정]
agent_type: "PPO"
model_class: "MaskablePPO"

# [환경 설정]
# 병렬로 실행할 환경의 개수입니다. (CPU 코어 수에 맞춰 8~16 권장)
# train.py 수정본에서 이 값을 읽어 SubprocVecEnv를 생성합니다.
num_cpu: 8  

# [경로 설정]
# 하드코딩 대신 설정 파일에서 경로를 제어합니다.
log_dir: "results/logs/PPO"
model_dir: "results/models/PPO"
model_name: "PPO_vs_bot"

# [학습 스케줄]
total_timesteps: 2000000  # 1,000만 스텝 (충분한 학습을 위해)
save_interval: 1000000     # 100만 스텝마다 모델 저장

# [모델 하이퍼파라미터 (MaskablePPO)]
model_hyperparameters:
  learning_rate: 0.0001     # 3e-4 (안정적인 학습률)
  n_steps: 8192             # 한 번 업데이트 전, 각 환경에서 수집할 스텝 수
  batch_size: 2048           # 미니 배치 크기 (64는 너무 작음, 512 권장)
  n_epochs: 10              # 수집된 데이터로 반복 학습할 횟수
  
  gamma: 0.999               # 할인율 (장기적 보상 중시)
  gae_lambda: 0.98          # Advantage 추정 파라미터
  clip_range: 0.2           # PPO 클리핑 범위 (정책 급변 방지)
  
  # [중요] 탐색 강제화
  # 0.0이면 로컬 미니멈(예: 특정 카드만 계속 구매)에 빠지기 쉽습니다.
  ent_coef: 0.05            
  
  vf_coef: 0.5              # 가치 함수 손실 가중치
  max_grad_norm: 0.5        # 기울기 폭발 방지
  
  policy_kwargs:
    net_arch: [256, 256]    # 신경망 구조 (Hidden Layer 크기)